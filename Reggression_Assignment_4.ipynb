{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is Lasso Regression, and How Does It Differ from Other Regression Techniques?\n",
        "# Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that adds an L1 regularization term to the loss function.\n",
        "# The objective is to minimize:\n",
        "# Loss function = RSS (Residual Sum of Squares) + λ * (sum of absolute values of coefficients)\n",
        "# Lasso can set some of the coefficients to zero, effectively performing feature selection by eliminating irrelevant predictors.\n",
        "# It differs from ordinary least squares regression (OLS) and Ridge Regression:\n",
        "# - OLS minimizes the residuals (without regularization).\n",
        "# - Ridge uses L2 regularization (penalizes large coefficients but does not eliminate them).\n",
        "# - Lasso uses L1 regularization (can shrink coefficients to zero, performing feature selection).\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "import numpy as np\n",
        "\n",
        "# Example of Lasso Regression\n",
        "X = np.array([[1], [2], [3], [4], [5]])  # Independent variable\n",
        "y = np.array([1, 2, 3, 4, 5])            # Dependent variable\n",
        "\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "lasso_model.fit(X, y)\n",
        "print(f\"Lasso Coefficients: {lasso_model.coef_}\")\n",
        "\n",
        "# Q2. What is the Main Advantage of Using Lasso Regression in Feature Selection?\n",
        "# The main advantage of Lasso Regression in feature selection is its ability to shrink the coefficients of some features to zero.\n",
        "# This means that Lasso automatically performs feature selection by identifying and eliminating irrelevant features.\n",
        "# This is particularly useful when there are many features, as Lasso can simplify the model and reduce overfitting.\n",
        "\n",
        "# Q3. How Do You Interpret the Coefficients of a Lasso Regression Model?\n",
        "# The interpretation of the coefficients in a Lasso Regression model is similar to that in ordinary linear regression:\n",
        "# - A positive coefficient means that as the corresponding independent variable increases, the dependent variable increases.\n",
        "# - A negative coefficient means that as the independent variable increases, the dependent variable decreases.\n",
        "# - The magnitude of the coefficient indicates the strength of the relationship between the independent and dependent variables.\n",
        "# Since Lasso may shrink some coefficients to zero, variables with zero coefficients are effectively excluded from the model.\n",
        "\n",
        "# Example interpretation\n",
        "print(f\"Lasso Intercept: {lasso_model.intercept_}\")\n",
        "print(f\"Lasso Coefficients: {lasso_model.coef_}\")\n",
        "\n",
        "# Q4. What are the Tuning Parameters That Can Be Adjusted in Lasso Regression, and How Do They Affect the Model's Performance?\n",
        "# The main tuning parameter in Lasso Regression is the regularization parameter, λ (alpha in sklearn).\n",
        "# - A larger value of λ increases regularization, resulting in more shrinkage of coefficients and more features being eliminated.\n",
        "# - A smaller value of λ reduces the effect of regularization, allowing the model to fit more closely to the data, but this can lead to overfitting.\n",
        "# Cross-validation is typically used to select the optimal value of λ.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example: Grid search for optimal alpha (regularization parameter)\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
        "lasso_grid = GridSearchCV(Lasso(), parameters, cv=5)\n",
        "lasso_grid.fit(X, y)\n",
        "print(f\"Best alpha value: {lasso_grid.best_params_['alpha']}\")\n",
        "\n",
        "# Q5. Can Lasso Regression be Used for Non-Linear Regression Problems? If Yes, How?\n",
        "# Lasso Regression is inherently a linear regression technique, meaning it models linear relationships between the features and the target.\n",
        "# However, it can be extended to handle non-linear relationships by using polynomial features or kernel methods:\n",
        "# - Polynomial features: By transforming the input features into higher-degree polynomials, Lasso can model non-linear relationships.\n",
        "# - Kernel methods: Non-linear transformations of the input data can be applied before applying Lasso to capture more complex patterns.\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Example: Using polynomial features with Lasso Regression\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "lasso_poly = Lasso(alpha=0.1)\n",
        "lasso_poly.fit(X_poly, y)\n",
        "print(f\"Lasso Coefficients with Polynomial Features: {lasso_poly.coef_}\")\n",
        "\n",
        "# Q6. What is the Difference Between Ridge Regression and Lasso Regression?\n",
        "# The main difference between Ridge and Lasso Regression lies in the type of regularization they use:\n",
        "# - Ridge Regression uses L2 regularization (penalizes the sum of squared coefficients), which shrinks coefficients but does not set them to zero.\n",
        "# - Lasso Regression uses L1 regularization (penalizes the sum of absolute values of coefficients), which can set some coefficients to zero, performing feature selection.\n",
        "# Lasso is more suited for sparse models where many features are irrelevant, while Ridge is better when all features contribute to the model.\n",
        "\n",
        "# Example of Ridge and Lasso comparison\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_model = Ridge(alpha=0.1)\n",
        "ridge_model.fit(X, y)\n",
        "print(f\"Ridge Coefficients: {ridge_model.coef_}\")\n",
        "print(f\"Lasso Coefficients: {lasso_model.coef_}\")\n",
        "\n",
        "# Q7. Can Lasso Regression Handle Multicollinearity in the Input Features? If Yes, How?\n",
        "# Lasso Regression can handle multicollinearity to some extent. Multicollinearity occurs when independent variables are highly correlated.\n",
        "# Lasso can help by selecting one feature from a group of correlated features and shrinking the others to zero.\n",
        "# This makes Lasso particularly useful when there are many correlated features, as it performs feature selection, eliminating unnecessary or redundant predictors.\n",
        "\n",
        "# Q8. How Do You Choose the Optimal Value of the Regularization Parameter (λ) in Lasso Regression?\n",
        "# The optimal value of the regularization parameter λ is typically chosen using cross-validation.\n",
        "# A smaller value of λ allows the model to fit the data more closely (risk of overfitting), while a larger λ shrinks coefficients more, potentially underfitting.\n",
        "# Grid search or randomized search is commonly used to find the best λ that balances bias and variance, ensuring the best model performance.\n",
        "\n",
        "# Example of cross-validation for tuning alpha\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Evaluate performance using cross-validation\n",
        "lasso_cv = Lasso(alpha=0.1)\n",
        "scores = cross_val_score(lasso_cv, X, y, cv=5)\n",
        "print(f\"Cross-validation scores: {scores.mean()}\")\n"
      ]
    }
  ]
}